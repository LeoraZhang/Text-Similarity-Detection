<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>34ce5adea5ae441083bbf23e143a3267</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell code" data-execution_count="1"
data-cell_id="ae76cdccc0b8499db2667492935981b1"
data-deepnote_cell_type="code" data-deepnote_to_be_reexecuted="false"
data-execution_millis="26696" data-execution_start="1676924628543"
data-source_hash="ad8d02ef" data-tags="[]">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Homework_3_Unstructured Data Analysis</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">## Name: Shu Zhang</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">## Topic: Text Similarity Comparison</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">## Objective: </span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">## In this case, my aim is to explore how to find which representitive with most similar political backgrounds/experiences to a certain representative based on comparison of their profile texts. </span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">## I think this can be used for parties/organizations to find candidates similar to an existed ideal candidate.</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">## Further, I think this can be used to couple matching/date app. </span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">## We can identify which two has the most similar requirements/experiences based on their profiles.</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Model</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">&#39;PartyRep.csv&#39;</span>, header<span class="op">=</span><span class="va">None</span>, names<span class="op">=</span>[<span class="st">&#39;text&#39;</span>])</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Pre-process the data</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>MAX_WORDS <span class="op">=</span> <span class="dv">10000</span>  <span class="co"># Word list size</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>MAX_LEN <span class="op">=</span> <span class="dv">200</span>  <span class="co"># Length of text</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> tf.keras.preprocessing.text.Tokenizer(num_words<span class="op">=</span>MAX_WORDS)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>tokenizer.fit_on_texts(data[<span class="st">&#39;text&#39;</span>].values)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>sequences <span class="op">=</span> tokenizer.texts_to_sequences(data[<span class="st">&#39;text&#39;</span>].values)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen<span class="op">=</span>MAX_LEN)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>embedding_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>input_layer <span class="op">=</span> Input(shape<span class="op">=</span>(MAX_LEN,))</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>embedding_layer <span class="op">=</span> Embedding(input_dim<span class="op">=</span>MAX_WORDS, output_dim<span class="op">=</span>embedding_size)(input_layer)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>conv_layer <span class="op">=</span> Conv1D(filters<span class="op">=</span><span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>)(embedding_layer)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>pooling_layer <span class="op">=</span> GlobalMaxPooling1D()(conv_layer)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>output_layer <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span>)(pooling_layer)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>input_layer, outputs<span class="op">=</span>output_layer)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>, loss<span class="op">=</span><span class="st">&#39;binary_crossentropy&#39;</span>, metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>x_train, x_test <span class="op">=</span> train_test_split(x, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>model.fit(x_train, np.zeros(<span class="bu">len</span>(x_train)), epochs<span class="op">=</span><span class="dv">10</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_data<span class="op">=</span>(x_test, np.zeros(<span class="bu">len</span>(x_test))))</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>embedding_model <span class="op">=</span> Model(inputs<span class="op">=</span>input_layer, outputs<span class="op">=</span>pooling_layer)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> embedding_model.predict(x)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>2023-02-20 20:23:48.573572: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-20 20:23:48.784953: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-02-20 20:23:48.790318: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcudart.so.11.0&#39;; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2023-02-20 20:23:48.790334: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-02-20 20:23:48.815326: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-02-20 20:23:50.306783: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libnvinfer.so.7&#39;; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-02-20 20:23:50.306859: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libnvinfer_plugin.so.7&#39;; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-02-20 20:23:50.306869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-02-20 20:23:53.190257: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcuda.so.1&#39;; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2023-02-20 20:23:53.190296: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)
2023-02-20 20:23:53.190315: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-b54f2ad1-8844-465d-bd84-0413e084b0a3): /proc/driver/nvidia/version does not exist
2023-02-20 20:23:53.190577: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/10
15/15 [==============================] - 2s 85ms/step - loss: 0.4154 - accuracy: 0.9372 - val_loss: 0.1414 - val_accuracy: 1.0000
Epoch 2/10
15/15 [==============================] - 2s 127ms/step - loss: 0.0593 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 1.0000
Epoch 3/10
15/15 [==============================] - 1s 84ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000
Epoch 4/10
15/15 [==============================] - 1s 72ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000
Epoch 5/10
15/15 [==============================] - 1s 68ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000
Epoch 6/10
15/15 [==============================] - 1s 67ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 9.4354e-04 - val_accuracy: 1.0000
Epoch 7/10
15/15 [==============================] - 1s 71ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 7.6633e-04 - val_accuracy: 1.0000
Epoch 8/10
15/15 [==============================] - 1s 70ms/step - loss: 9.5967e-04 - accuracy: 1.0000 - val_loss: 6.3615e-04 - val_accuracy: 1.0000
Epoch 9/10
15/15 [==============================] - 1s 68ms/step - loss: 7.9766e-04 - accuracy: 1.0000 - val_loss: 5.3869e-04 - val_accuracy: 1.0000
Epoch 10/10
15/15 [==============================] - 1s 67ms/step - loss: 6.8813e-04 - accuracy: 1.0000 - val_loss: 4.5810e-04 - val_accuracy: 1.0000
19/19 [==============================] - 0s 11ms/step
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="3"
data-cell_id="9f1681fc364a4e5aba3241172a09131c"
data-deepnote_cell_type="code" data-deepnote_to_be_reexecuted="false"
data-execution_millis="243" data-execution_start="1676925188263"
data-source_hash="1f4bdf90" data-tags="[]">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">## I calculated the similarity of those profiles to see which two representatives might have most similar political background/experiences. </span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co">## Here&#39;s the example to see the similarity between the first representative&#39;s profile and others&#39; profiles.</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">## And I can find that text#31 is most similar to text#1.</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Use well-trained embedding_model to extract text characteristics</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> embedding_model.predict(x)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the cosine_similarity of each pair of texts</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>similarity_matrix <span class="op">=</span> cosine_similarity(embeddings)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out the similarity of the first text and other texts</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>text1_index <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>text1_similarities <span class="op">=</span> similarity_matrix[text1_index]</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Text 1 similarities: </span><span class="sc">{</span>text1_similarities<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the most similar one </span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>most_similar_index <span class="op">=</span> text1_similarities.argsort()[<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Text 1 most similar to text </span><span class="sc">{</span>most_similar_index<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>19/19 [==============================] - 0s 9ms/step
Text 1 similarities: [1.0000001  0.8596003  0.85121155 0.8610106  0.85663915 0.8579718
 0.8172304  0.85458165 0.7153534  0.8129129  0.81471163 0.7187015
 0.7154286  0.8601674  0.83915967 0.8601982  0.72597075 0.85657555
 0.8574427  0.7273834  0.8421465  0.80726373 0.8603945  0.7287773
 0.8596496  0.8598249  0.8533907  0.8529233  0.84860814 0.8605442
 0.7167251  0.8888499  0.85821164 0.85334784 0.75016564 0.8600415
 0.7160087  0.8601597  0.86036444 0.85001886 0.85290897 0.86087096
 0.8530399  0.742533   0.7308064  0.86086714 0.84111446 0.8534145
 0.8586418  0.8543836  0.84252375 0.8126264  0.85177857 0.8437992
 0.72034115 0.8163173  0.8543512  0.7179657  0.72761965 0.8554464
 0.8567327  0.86296374 0.7257651  0.7081171  0.8467201  0.86148775
 0.85492355 0.8602006  0.84375536 0.85267466 0.84811884 0.8535497
 0.8538918  0.8549813  0.71975595 0.85591614 0.7260118  0.8524309
 0.85337156 0.86160713 0.857939   0.72905743 0.8545565  0.85878634
 0.8500428  0.8581771  0.85127205 0.84852797 0.8576017  0.84869635
 0.85405767 0.8513802  0.8536099  0.8557791  0.84892005 0.85743934
 0.8570121  0.856055   0.85597146 0.8593224  0.85381174 0.85820776
 0.8584632  0.8516059  0.86363244 0.8274278  0.86172813 0.73014164
 0.8479347  0.8546752  0.85678464 0.85221547 0.85171896 0.8550508
 0.8573518  0.86095405 0.86121595 0.85398954 0.85160935 0.86088884
 0.8582558  0.727893   0.81960857 0.8576526  0.8640616  0.85710627
 0.8566277  0.73685235 0.85863346 0.85419935 0.8495656  0.7045265
 0.85385114 0.85775775 0.8646878  0.8390758  0.8524247  0.7274482
 0.8424735  0.7188561  0.8117333  0.82102966 0.85751843 0.7248786
 0.72409153 0.8392528  0.726788   0.85568786 0.8588778  0.85273224
 0.86437666 0.8488527  0.71946174 0.85373056 0.857433   0.7293691
 0.8112931  0.86720407 0.8554747  0.85606414 0.8525684  0.85141563
 0.72464985 0.72994107 0.8557014  0.8476333  0.85050684 0.85235876
 0.85431665 0.86250234 0.85403585 0.8487354  0.7169741  0.85456556
 0.8556205  0.72034746 0.8567311  0.8618868  0.85741323 0.85545474
 0.8592423  0.8502574  0.857061   0.8546069  0.8541453  0.8552719
 0.8593349  0.8532553  0.72685695 0.83865887 0.8600539  0.85028875
 0.8576766  0.82320607 0.85123205 0.729142   0.8532911  0.85704005
 0.8560289  0.7251345  0.8540926  0.72696346 0.8354292  0.82017803
 0.8535283  0.855139   0.7177735  0.8586134  0.85916847 0.85734254
 0.8625167  0.72878444 0.85132813 0.85876036 0.8498628  0.8566165
 0.8576702  0.7790834  0.8473684  0.85203475 0.85485685 0.7170648
 0.85206366 0.73579335 0.8593206  0.8552635  0.72730595 0.8331267
 0.75141925 0.8521047  0.8561303  0.847979   0.85281235 0.7232194
 0.85583365 0.84404385 0.85395014 0.8618411  0.7238051  0.85423064
 0.84902495 0.8668045  0.861217   0.7582926  0.8583981  0.85515434
 0.73929554 0.7363848  0.858843   0.72283554 0.8475965  0.7184973
 0.85299385 0.85146636 0.85924846 0.8521767  0.85473615 0.8580151
 0.85644746 0.7226003  0.8548269  0.8567259  0.8563772  0.8536786
 0.77075547 0.7214356  0.7332026  0.8570836  0.7099823  0.85401726
 0.85845894 0.8667945  0.8556544  0.85783386 0.85896635 0.8564606
 0.80496705 0.7355658  0.84969544 0.8559388  0.85634285 0.72347194
 0.7290088  0.85037297 0.8540317  0.85634613 0.7320045  0.8587176
 0.8499631  0.85188514 0.85880196 0.85055    0.85895246 0.850386
 0.8525176  0.8495985  0.72725827 0.8489121  0.8553256  0.857835
 0.85656893 0.8490169  0.85869193 0.86014414 0.8523359  0.8526817
 0.8540351  0.83639514 0.8140318  0.85318065 0.85701346 0.86269027
 0.71014476 0.8571529  0.85892016 0.71505594 0.72437376 0.8355633
 0.8558757  0.8594977  0.85315067 0.8504248  0.71359444 0.7226763
 0.85965115 0.8550406  0.71748626 0.85881746 0.71725917 0.8520488
 0.844399   0.85936433 0.72943443 0.7257642  0.85993344 0.72178656
 0.7238513  0.8575386  0.8577548  0.7109954  0.7195217  0.85485363
 0.8578267  0.846859   0.8531975  0.849629   0.85547775 0.716519
 0.8547547  0.85664475 0.85940635 0.7375891  0.84718484 0.8571365
 0.85027903 0.85197896 0.8562779  0.7313625  0.7326522  0.72201806
 0.8571685  0.85150564 0.85538197 0.8491379  0.85656995 0.85208505
 0.7284854  0.8604852  0.7121137  0.8503093  0.8511864  0.8536662
 0.8580377  0.7205011  0.83972216 0.7072229  0.85185194 0.85585654
 0.85991937 0.73827904 0.82807547 0.85867846 0.85583305 0.8583542
 0.72191626 0.8595482  0.71326256 0.81669766 0.8516073  0.8619196
 0.85811496 0.8386729  0.8507396  0.85866386 0.72083586 0.8595124
 0.8547791  0.85016227 0.8587209  0.8607175  0.85459137 0.86899287
 0.8550469  0.854757   0.85552156 0.85387313 0.85290545 0.7149586
 0.7223236  0.8479974  0.85972357 0.8412063  0.8539895  0.8571548
 0.7224094  0.84708595 0.8187908  0.71682847 0.856975   0.85894465
 0.72125727 0.85524875 0.85539234 0.851761   0.85458153 0.85599864
 0.8431631  0.7204271  0.85754156 0.85405743 0.855904   0.72039616
 0.85647756 0.83333015 0.8569657  0.85330456 0.85548925 0.8541182
 0.858384   0.8530733  0.8580641  0.7089797  0.8584453  0.8332933
 0.85128343 0.85216457 0.7207844  0.85647595 0.85687023 0.8604154
 0.8562403  0.7214231  0.71259624 0.7282691  0.8147025  0.8553539
 0.83019423 0.8442854  0.73231816 0.8324716  0.84933555 0.85216254
 0.8585564  0.8570044  0.7165108  0.8538311  0.8567141  0.74394995
 0.8597177  0.84775126 0.8533688  0.8437739  0.8566652  0.7127942
 0.8534524  0.8487698  0.86311305 0.85912037 0.87064296 0.7480962
 0.84867746 0.8515076  0.85583264 0.8443181  0.8602976  0.84981894
 0.8589447  0.718545   0.73276746 0.8526655  0.8506721  0.8486072
 0.72409254 0.8535173  0.85942715 0.8521097  0.8472908  0.8542762
 0.85905206 0.85064054 0.84967613 0.8564687  0.72538954 0.8596149
 0.85634923 0.8525365  0.8666795  0.8449941  0.8609182  0.8519022
 0.75076294 0.8548024  0.8616155  0.8363348  0.85465974 0.80649406
 0.8548091  0.85689545 0.84320676 0.8446284  0.8186464  0.8546969
 0.72930366 0.8605349  0.8554945  0.80682594 0.85988873 0.85699785
 0.7163993  0.8552286  0.7089567  0.85285026 0.7041787  0.85232246
 0.8591508  0.8587498  0.852634   0.74689096 0.7236007  0.86074597
 0.8584493  0.86095166 0.8544515  0.8589376  0.8613189  0.8534905
 0.85589635 0.8587945  0.85671103 0.85972625 0.8562792  0.84877473
 0.8506778  0.8496353  0.81619555 0.8526354  0.8533396  0.8529367
 0.83949363 0.7337531  0.8583881  0.8544987  0.86117    0.85530025
 0.8639414  0.8565957  0.8570262  0.8563617  0.85870403 0.854177
 0.8493961  0.86040044 0.85533196 0.7267753  0.8537609  0.85910386
 0.8534914  0.84543854 0.8585017  0.81201154 0.84806937 0.72813046
 0.8626036  0.72707695 0.8597249  0.7298366  0.8583173  0.8603464
 0.85237676 0.8562762  0.8585577  0.8511446  0.86440974 0.85094774
 0.7142578  0.8551828  0.72245246 0.86173743]
Text 1 most similar to text 31
</code></pre>
</div>
</div>
<div class="cell markdown" data-created_in_deepnote_cell="true"
data-deepnote_cell_type="markdown" data-tags="[]">
<p><a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b54f2ad1-8844-465d-bd84-0413e084b0a3' target="_blank">
<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' >
</img> Created in
<span style='font-weight:600;margin-left:4px;'>Deepnote</span></a></p>
</div>
</body>
</html>
